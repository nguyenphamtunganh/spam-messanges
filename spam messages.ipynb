{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['v1', 'v2', '', '', ''],\n",
       " ['ham',\n",
       "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'Ok lar... Joking wif u oni...', '', '', ''],\n",
       " ['spam',\n",
       "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'U dun say so early hor... U c already then say...', '', '', '']]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here, I will train the machine to classify spam and ham messages by Bayes' rule \n",
    "# The dataset was downloaded from kaggle.com\n",
    "\n",
    "import csv\n",
    "f = open('spam.csv', 'r')\n",
    "csvreader = csv.reader(f)\n",
    "data = list(csvreader)\n",
    "data[0:5]\n",
    "\n",
    "# First look at the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ham',\n",
       "  'Go until jurong point, crazy.. Available only in bugis n great world la e buffet... Cine there got amore wat...',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'Ok lar... Joking wif u oni...', '', '', ''],\n",
       " ['spam',\n",
       "  \"Free entry in 2 a wkly comp to win FA Cup final tkts 21st May 2005. Text FA to 87121 to receive entry question(std txt rate)T&C's apply 08452810075over18's\",\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'U dun say so early hor... U c already then say...', '', '', ''],\n",
       " ['ham',\n",
       "  \"Nah I don't think he goes to usf, he lives around here though\",\n",
       "  '',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = data[1:len(data)]  # Removing the heading\n",
    "data[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
     ]
    }
   ],
   "source": [
    "import string\n",
    "print(string.punctuation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[['ham',\n",
       "  'go until jurong point crazy available only in bugis n great world la e buffet cine there got amore wat',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'ok lar joking wif u oni', '', '', ''],\n",
       " ['spam',\n",
       "  'free entry in 2 a wkly comp to win fa cup final tkts 21st may 2005 text fa to 87121 to receive entry questionstd txt ratetcs apply 08452810075over18s',\n",
       "  '',\n",
       "  '',\n",
       "  ''],\n",
       " ['ham', 'u dun say so early hor u c already then say', '', '', ''],\n",
       " ['ham',\n",
       "  'nah i dont think he goes to usf he lives around here though',\n",
       "  '',\n",
       "  '',\n",
       "  '']]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Filter out the punctuation and convert all words to lowercase\n",
    "\n",
    "table = str.maketrans('', '', string.punctuation)\n",
    "data1 = []\n",
    "for row in data:\n",
    "    strippled = [word.translate(table) for word in row]\n",
    "    strippled_lower = [word.lower() for word in strippled]\n",
    "    data1.append(strippled_lower)\n",
    "data1[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'in', '2', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', '21st', 'may', '2005', 'text', 'fa', 'to', '87121', 'to', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply', '08452810075over18s'], ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say'], ['nah', 'i', 'dont', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']]\n",
      "['ham', 'ham', 'spam', 'ham', 'ham']\n"
     ]
    }
   ],
   "source": [
    "# Split 2nd element in every row by whitespace\n",
    "\n",
    "data2 = []\n",
    "label = []\n",
    "for row in data1:\n",
    "        temp = row[1].split(' ')\n",
    "        data2.append(temp)\n",
    "        label.append(row[0])\n",
    "print(data2[0:5])\n",
    "print(label[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'until', 'jurong', 'point', 'crazy', 'available', 'only', 'in', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'there', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'in', 'a', 'wkly', 'comp', 'to', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'to', 'to', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply'], ['u', 'dun', 'say', 'so', 'early', 'hor', 'u', 'c', 'already', 'then', 'say'], ['nah', 'i', 'dont', 'think', 'he', 'goes', 'to', 'usf', 'he', 'lives', 'around', 'here', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Removing all the number\n",
    "\n",
    "data3 = []\n",
    "for row in data2:\n",
    "    temp1 = [word for word in row if word.isalpha()]\n",
    "    data3.append(temp1)\n",
    "print(data3[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter out stop words \n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = [word.translate(table) for word in stop_words]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazy', 'available', 'bugis', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amore', 'wat'], ['ok', 'lar', 'joking', 'wif', 'u', 'oni'], ['free', 'entry', 'wkly', 'comp', 'win', 'fa', 'cup', 'final', 'tkts', 'may', 'text', 'fa', 'receive', 'entry', 'questionstd', 'txt', 'ratetcs', 'apply'], ['u', 'dun', 'say', 'early', 'hor', 'u', 'c', 'already', 'say'], ['nah', 'think', 'goes', 'usf', 'lives', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "data4 = []\n",
    "for row in data3:\n",
    "    temp = [word for word in row if not word in stop_words]\n",
    "    data4.append(temp)                \n",
    "print(data4[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['free', 'entri', 'wkli', 'comp', 'win', 'fa', 'cup', 'final', 'tkt', 'may', 'text', 'fa', 'receiv', 'entri', 'questionstd', 'txt', 'ratetc', 'appli'], ['u', 'dun', 'say', 'earli', 'hor', 'u', 'c', 'alreadi', 'say'], ['nah', 'think', 'goe', 'usf', 'live', 'around', 'though']]\n"
     ]
    }
   ],
   "source": [
    "# Stemming of words\n",
    "\n",
    "data_clean = []\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "porter = PorterStemmer()\n",
    "for row in data4:\n",
    "    temp = [porter.stem(word) for word in row]\n",
    "    data_clean.append(temp)\n",
    "print(data_clean[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will dvivide the data_clean by two subsets: training set (80%) and test set (20%)\n",
    "\n",
    "training_set = data_clean[:4459]\n",
    "label_training = label[:4459]\n",
    "test_set = data_clean[4459:]\n",
    "label_test = label[4459:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6032"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a dictionary from the training set\n",
    "\n",
    "dict = {}\n",
    "for row in training_set:\n",
    "    for word in row:\n",
    "        if word in dict:\n",
    "            dict[word] += 1\n",
    "        else:\n",
    "            dict[word] = 1\n",
    "v = len(dict)\n",
    "v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['go', 'jurong', 'point', 'crazi', 'avail', 'bugi', 'n', 'great', 'world', 'la', 'e', 'buffet', 'cine', 'got', 'amor', 'wat'], ['ok', 'lar', 'joke', 'wif', 'u', 'oni'], ['u', 'dun', 'say', 'earli', 'hor', 'u', 'c', 'alreadi', 'say'], ['nah', 'think', 'goe', 'usf', 'live', 'around', 'though'], ['even', 'brother', 'like', 'speak', 'treat', 'like', 'aid', 'patent']]\n"
     ]
    }
   ],
   "source": [
    "# Then separate the training set into ham and spam messages\n",
    "\n",
    "ham = []\n",
    "spam = []\n",
    "for i, element in enumerate(label_training):\n",
    "    if label_training[i] == 'ham':\n",
    "        ham.append(training_set[i])\n",
    "    else:\n",
    "        spam.append(training_set[i])\n",
    "print(ham[0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.009106119202391728,\n",
       " 5.4856139773444146e-05,\n",
       " 0.0004114210483008311,\n",
       " 0.00024685262898049864,\n",
       " 0.0003565649085273869]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Training the data by Bayes classifier\n",
    "# Our objective is to find the set of lambda for each ham and spam set which serves as p(xi|c)\n",
    "# Caculate P(xi|c)\n",
    "\n",
    "count = 0\n",
    "for row in ham:\n",
    "    for word in row:\n",
    "        count += 1\n",
    "        \n",
    "lambda_head_ham = []\n",
    "for word in dict:\n",
    "    total = 0\n",
    "    for row in ham:\n",
    "        for word1 in row:\n",
    "            if word1 == word:\n",
    "                total += 1\n",
    "    prob = (total + 1)/(count + v)\n",
    "    lambda_head_ham.append(prob)\n",
    "lambda_head_ham[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0008502701664883843,\n",
       " 2.7428069886722073e-05,\n",
       " 0.0002742806988672207,\n",
       " 0.00010971227954688829,\n",
       " 0.00010971227954688829]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lambda_head_spam = []\n",
    "for word in dict:\n",
    "    total = 0\n",
    "    for row in spam:\n",
    "        for word1 in row:\n",
    "            if word1 == word:\n",
    "                total += 1\n",
    "    prob = (total + 1)/(count + v)\n",
    "    lambda_head_spam.append(prob)\n",
    "lambda_head_spam[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate P(spam) and P(ham)\n",
    "\n",
    "prob_ham = len(ham)/len(training_set)\n",
    "prob_spam = len(spam)/len(training_set)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lastly, we will test our classifier\n",
    "# Mapping all test set to the matrix\n",
    "\n",
    "mapping_test = []\n",
    "for row in test_set:\n",
    "    mapping = []\n",
    "    for word in dict:\n",
    "        temp = 0\n",
    "        for word1 in row:\n",
    "            if word1 == word:\n",
    "                temp += 1\n",
    "        mapping.append(temp)\n",
    "    mapping_test.append(mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "mapping_test = np.array(mapping_test)\n",
    "lambda_head_ham = np.array(lambda_head_ham)\n",
    "lambda_head_spam = np.array(lambda_head_spam)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Caculate P(spam|xi)\n",
    "prob_spam1 = np.log(prob_spam) + sum(np.transpose(np.log(lambda_head_spam ** mapping_test)))\n",
    "# Caculate P(ham|xi)\n",
    "prob_ham1 = np.log(prob_ham) + sum(np.transpose(np.log(lambda_head_ham ** mapping_test)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare p(spam|xi) and p(ham|xi) and decide the label for the text\n",
    "\n",
    "label_decide  = []\n",
    "temp3 = prob_spam1 > prob_ham1\n",
    "for row in temp3:\n",
    "    if row == True:\n",
    "        label_decide.append('spam')\n",
    "    else:\n",
    "        label_decide.append('ham')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ True,  True,  True, ...,  True,  True,  True])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_decide = np.array(label_decide)\n",
    "label_test = np.array(label_test)\n",
    "decide = label_decide == label_test\n",
    "decide"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "95.8670260557053"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "count_label = 0\n",
    "for row in decide:\n",
    "    if row == True:\n",
    "        count_label += 1\n",
    "right_predictation = count_label/len(test_set) * 100\n",
    "right_predictation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As we can seen, Bayes classifier yields an impressive result (95,86%) !!!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
